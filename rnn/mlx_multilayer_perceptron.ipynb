{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Layer Perceptron\n",
    "\n",
    "In this example we’ll learn to use mlx.nn by implementing a simple multi-layer perceptron to classify MNIST.\n",
    "\n",
    "As a first step import the MLX packages we need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlx.core as mx\n",
    "import mlx.nn as nn\n",
    "import mlx.optimizers as optim\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is defined as the MLP class which inherits from mlx.nn.Module. We follow the standard idiom to make a new module:\n",
    "\n",
    "Define an __init__ where the parameters and/or submodules are setup. See the Module class docs for more information on how mlx.nn.Module registers parameters.\n",
    "Define a __call__ where the computation is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers: int, input_dim: int, hidden_dim: int, output_dim: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        layer_sizes = [input_dim] + [hidden_dim] * num_layers + [output_dim]\n",
    "        self.layers = [\n",
    "            nn.Linear(idim, odim)\n",
    "            for idim, odim in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for l in self.layers[:-1]:\n",
    "            x = mx.maximum(l(x), 0.0)\n",
    "        return self.layers[-1](x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the loss function which takes the mean of the per-example cross entropy loss. The mlx.nn.losses sub-package has implementations of some commonly used loss functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, X, y):\n",
    "    return mx.mean(nn.losses.cross_entropy(model(X), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need a function to compute the accuracy of the model on the validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_fn(model, X, y):\n",
    "    return mx.mean(mx.argmax(model(X), axis=1) == y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, setup the problem parameters and load the data. To load the data, you need our mnist data loader, which we will import as mnist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 2\n",
    "hidden_dim = 32\n",
    "num_classes = 10\n",
    "batch_size = 256\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-1\n",
    "\n",
    "# Load the data\n",
    "import mnist\n",
    "train_images, train_labels, test_images, test_labels = map(\n",
    "    mx.array, mnist.mnist()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we’re using SGD, we need an iterator which shuffles and constructs minibatches of examples in the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_iterate(batch_size, X, y):\n",
    "    perm = mx.array(np.random.permutation(y.size))\n",
    "    for s in range(0, y.size, batch_size):\n",
    "        ids = perm[s : s + batch_size]\n",
    "        yield X[ids], y[ids]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we put it all together by instantiating the model, the mlx.optimizers.SGD optimizer, and running the training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[take_along_axis] Indices of dimension 2 does not match array of dimension 3.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m X, y \u001b[38;5;129;01min\u001b[39;00m batch_iterate(batch_size, train_images, train_labels):\n\u001b[0;32m---> 14\u001b[0m         loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mloss_and_grad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# Update the optimizer state and model parameters\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;66;03m# in a single call\u001b[39;00m\n\u001b[1;32m     18\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mupdate(model, grads)\n",
      "File \u001b[0;32m~/mlx-env/lib/python3.9/site-packages/mlx/nn/utils.py:30\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.wrapped_value_grad_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped_value_grad_fn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 30\u001b[0m     value, grad \u001b[38;5;241m=\u001b[39m \u001b[43mvalue_grad_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainable_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value, grad\n",
      "File \u001b[0;32m~/mlx-env/lib/python3.9/site-packages/mlx/nn/utils.py:25\u001b[0m, in \u001b[0;36mvalue_and_grad.<locals>.inner_fn\u001b[0;34m(params, *args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner_fn\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     24\u001b[0m     model\u001b[38;5;241m.\u001b[39mupdate(params)\n\u001b[0;32m---> 25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(model, X, y)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_fn\u001b[39m(model, X, y):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mx\u001b[38;5;241m.\u001b[39mmean(\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/mlx-env/lib/python3.9/site-packages/mlx/nn/losses.py:33\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(logits, targets, axis, reduction)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcross_entropy\u001b[39m(\n\u001b[1;32m     18\u001b[0m     logits: mx\u001b[38;5;241m.\u001b[39marray, targets: mx\u001b[38;5;241m.\u001b[39marray, axis: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, reduction: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnone\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     19\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m mx\u001b[38;5;241m.\u001b[39marray:\n\u001b[1;32m     20\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;03m    Computes the cross entropy loss between logits and targets.\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m        mx.array: The computed cross entropy loss.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mmx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake_along_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     34\u001b[0m     loss \u001b[38;5;241m=\u001b[39m mx\u001b[38;5;241m.\u001b[39mlogsumexp(logits, axis\u001b[38;5;241m=\u001b[39maxis) \u001b[38;5;241m-\u001b[39m score\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _reduce(loss, reduction)\n",
      "\u001b[0;31mValueError\u001b[0m: [take_along_axis] Indices of dimension 2 does not match array of dimension 3."
     ]
    }
   ],
   "source": [
    "# Load the model\n",
    "model = MLP(num_layers, train_images.shape[-1], hidden_dim, num_classes)\n",
    "mx.eval(model.parameters())\n",
    "\n",
    "# Get a function which gives the loss and gradient of the\n",
    "# loss with respect to the model's trainable parameters\n",
    "loss_and_grad_fn = nn.value_and_grad(model, loss_fn)\n",
    "\n",
    "# Instantiate the optimizer\n",
    "optimizer = optim.SGD(learning_rate=learning_rate)\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    for X, y in batch_iterate(batch_size, train_images, train_labels):\n",
    "        loss, grads = loss_and_grad_fn(model, X, y)\n",
    "\n",
    "        # Update the optimizer state and model parameters\n",
    "        # in a single call\n",
    "        optimizer.update(model, grads)\n",
    "\n",
    "        # Force a graph evaluation\n",
    "        mx.eval(model.parameters(), optimizer.state)\n",
    "\n",
    "    accuracy = eval_fn(model, test_images, test_labels)\n",
    "    print(f\"Epoch {e}: Test accuracy {accuracy.item():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model should train to a decent accuracy (about 95%) after just a few passes over the training set. The full example is available in the MLX GitHub repo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlx-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
